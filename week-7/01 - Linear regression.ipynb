{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190aede4",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-chick",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Motivation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exploratory-Analysis\" data-toc-modified-id=\"Exploratory-Analysis-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Exploratory Analysis</a></span></li></ul></li><li><span><a href=\"#Simple-linear-regression\" data-toc-modified-id=\"Simple-linear-regression-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Simple linear regression</a></span></li><li><span><a href=\"#How-good-is-our-model?\" data-toc-modified-id=\"How-good-is-our-model?-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>How good is our model?</a></span></li><li><span><a href=\"#Calculate-the-R2-of-the-model\" data-toc-modified-id=\"Calculate-the-R2-of-the-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Calculate the R2 of the model</a></span></li><li><span><a href=\"#Linear-regression-with-sklearn\" data-toc-modified-id=\"Linear-regression-with-sklearn-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Linear regression with sklearn</a></span></li><li><span><a href=\"#Linear-regression-with-statsmodels\" data-toc-modified-id=\"Linear-regression-with-statsmodels-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Linear regression with statsmodels</a></span></li><li><span><a href=\"#OLS-Concepts\" data-toc-modified-id=\"OLS-Concepts-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>OLS Concepts</a></span></li><li><span><a href=\"#Multiple-Linear-Regression\" data-toc-modified-id=\"Multiple-Linear-Regression-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Multiple Linear Regression</a></span></li><li><span><a href=\"#Categorical-variables\" data-toc-modified-id=\"Categorical-variables-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Categorical variables</a></span></li><li><span><a href=\"#Linear-model-extensions\" data-toc-modified-id=\"Linear-model-extensions-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Linear model extensions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Challenging-the-additive-assumption:-synergy\" data-toc-modified-id=\"Challenging-the-additive-assumption:-synergy-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Challenging the additive assumption: synergy</a></span></li></ul></li><li><span><a href=\"#Let's-do-it\" data-toc-modified-id=\"Let's-do-it-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Let's do it</a></span><ul class=\"toc-item\"><li><span><a href=\"#$R^2$-Adjusted\" data-toc-modified-id=\"$R^2$-Adjusted-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>$R^2$ Adjusted</a></span></li></ul></li><li><span><a href=\"#Potential-problems-in-linear-regression\" data-toc-modified-id=\"Potential-problems-in-linear-regression-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Potential problems in linear regression</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Further-Materials\" data-toc-modified-id=\"Further-Materials-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Further Materials</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de361d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "557d273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Viz\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config Inlinebackend.figure_format = 'retina'\n",
    "sns.set_context(\"poster\")\n",
    "sns.set(rc={\"figure.figsize\": (15.,8.)})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Modelling libraries\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression #sci-kit for learning, science-kit, sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a25c2b2",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b1235e",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "We have 100 students, and we know:\n",
    " * How many hours they studied for their exam\n",
    " * The grade they have obtained (from 0 to 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f05e727",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95450218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dca9195",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "We would like to understand the relationship $$grade = f(hours)$$\n",
    "\n",
    "To be able to **predict the expected grade** that we will obtain when studying a certain number of hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d9e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "179b34b5",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Let's try a $$Y = m * X + n$$ linear regression\n",
    "$m$ is the slope\n",
    "$n$ is the value of $Y$ when $X=0$\n",
    "And we have to:\n",
    "$$grade = m * hours + n$$\n",
    "We want to find $m$ and $n$ that *best* model our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9babbd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b0bdb01",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Which has worked better? How can we measure the error of the models?\n",
    "\n",
    "We can subtract our prediction from the grade to see what we have done wrong in each one and make the average.... we use the absolute value to remove the negative values ​​and then we talk about this error, which has a name and it is MAE (mean absolutely error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115a9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fbd1f9f",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "What is going to be our goal? Python and its libraries do this for us, it calculates the slope and the intercept to be able to make \"predictions\" of unknown data from data that we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d786c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc3ad931",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Simple linear regression\n",
    "\n",
    "Simple linear regression a statistical model that assumes a linear relationship between a predictor and a target variable. Mathematically, it can be expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-monthly",
   "metadata": {},
   "source": [
    "![formula](../images/formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05109428",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "If we dig a little deeper, we can find this other expression:\n",
    "\n",
    " $$ Y = \\beta_0 +  \\beta_1 X + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "* $X$ = predictor variable\n",
    "* $Y$ = target variable\n",
    "* $\\beta_0$ = intercept\n",
    "* $\\beta_1$ = slope / slope\n",
    "* $\\epsilon$ = noise (gaussian)\n",
    "\n",
    "\n",
    "The above equation is known as the *population regression line*.\n",
    "The simple linear regression line usually has the form shown in the formula above, where β0 and β1 are unknown constants, representing the intercept and slope of the regression line, respectively.\n",
    "\n",
    "The intercept is the value of the dependent variable (Y) when the independent variable (X) has a value of zero (0). The slope is a measure of the rate at which the dependent variable (Y) changes when the independent variable (X) changes by one (1). The unknown constants are called the coefficients or parameters of the model. This form of the regression line is sometimes known as a population regression line and, as a probabilistic model, it roughly fits the data set hence the use of the symbol (≈) in the image. The model is called probabilistic because it does not model all the variability of the dependent variable (Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26bfe20",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## How good is our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c62da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4854b3bf",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The numerical difference between the *least squares regression line* and the actual value is called the *residual* , and it represents the error in the estimate: $e = y_i - \\hat{y}$.\n",
    "The regression line minimized the **Residual Sum of Squares** (RSS)\n",
    "\n",
    "The residual sum of squares measures the amount of error remaining between the regression function and the data set. a smaller residual sum of squares represents a regression function. The residual sum of squares, also known as the sum of squared residuals, essentially determines how well a regression model explains or represents the data in the model.\n",
    "\n",
    "$$RSS = e_1^2 + e_2^2 + \\dots + e_n ^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a76779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a34f686d",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "If we only used the mean as the predicted value for each prediction, the error we would make is (**total sum of squares**)\n",
    "\n",
    "$$TSS=\\Sigma(y_i - \\bar{y}_i)^2$$\n",
    "Let's consider this our starting point, make a prediction and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46161a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c20aee76",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Recall that linear regression coefficients minimize the $RSS=Sigma(y_i - \\hat{y_i})^2$, that is, the amount of variability that remains unexplained after running the regression. The [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination):\n",
    "\n",
    "$$R^2 = \\frac{TSS -RSS}{TSS} = 1-\\frac{RSS}{TSS}$$\n",
    "\n",
    "measures the \"*proportion of variability in Y that can be explained by X*\". It is a measure of the linear relationship that exists between $X$ and $y$.\n",
    "\n",
    "**Note:** in the case of simple linear regression, the $R^2$ coefficient is nothing more than the square of the *Pearson* correlation coefficient that we already know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7071446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4816a59c",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Calculate the R2 of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2489fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "107c1a48",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "$R^2$ measures how good our regression model is. The bigger the better. It is a value between 0 and 1\n",
    "\n",
    "**NOTE**: it is computable for any model, it does not matter if it is linear or not. Only the actual and predicted values ​​are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170b9717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73807966",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Linear regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f5e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a954f830",
   "metadata": {},
   "source": [
    "# 1. Select the data: `my_variables = X, y` # independant X, dependant y. target variable -> y (grades)\n",
    "# 2. Fit the model to the data: `my_model = lr.fit(X, y)`\n",
    "# 3. Predict with the model fitted: `predicted_y = lr.predict(X)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "similar-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20e9cc",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Calculation of the absolute mean error, the mean square error and the mean square error\n",
    "\n",
    "- **MAE** is the easiest to understand, because it is the mean error.\n",
    "- **The MSE** is more popular than the MAE, because the MSE accounts for larger errors, which is often useful in the real world.\n",
    "- **RMSE** is even more popular than MSE, it is the square root of the MSE and measures the standard deviation of the residuals.\n",
    "\n",
    "These are all **loss functions**: we want to minimize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f27fb4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(df.mark, df.prediction_through_scikitlerarn)\n",
    "mse = metrics.mean_squared_error(df.mark, df.prediction_through_scikitlerarn)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = metrics.r2_score(df.mark, df.prediction_through_scikitlerarn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "181b7386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE is: 11.402698299019335\n",
      "MSE is: 189.31609105092303\n",
      "RMSE is: 13.759218402617318\n",
      "r2 is: 0.7518130774464549\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAE is: {mae}\")\n",
    "print(f\"MSE is: {mse}\")\n",
    "print(f\"RMSE is: {rmse}\")\n",
    "print(f\"r2 is: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-spanking",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Read more about MAE, MSE, RMSE AND R2 [here](http://medium.com/analytics-vidhya/mae-mse-rmse-coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb9bf74",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Linear regression with statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be450c43",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "A bit of [documentation](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html)\n",
    "\n",
    "And [this article](https://jyotiyadav99111.medium.com/statistics-how-should-i-interpret-results-of-ols-3bde1ebeec01) that summarizes how to interpret OLS summary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdbd743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cb6310f",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## OLS Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57397238",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "- <b>R2</b> : The coefficient of determination measures how much of the variation of 𝑦 is explained by the model.\n",
    "If the variance of the errors or residuals 𝜎2𝑒 is zero, the model explains 100% of the variable 𝑦. If 𝜎2𝑒 is equal to the variance of 𝑦 the model explains nothing and 𝑅2 is equal to zero.\n",
    "\n",
    "\n",
    "- <b>𝑅¯2 </b> : The adjusted correlation coefficient 𝑅¯2 corrects the value of 𝑅2 by the number of variables 𝑘 (equal to 2 for the analyzed case) and the amount of data 𝑁\n",
    "\n",
    "- <b>P value </b> The p-value for each term tests the null hypothesis that the coefficient is equal to zero (has no effect). A low p-value (< 0.05) indicates that you can reject the null hypothesis. ... Typically p-values ​​are used to determine which terms should be kept in the regression model.\n",
    "\n",
    "\n",
    "Description of the p value:\n",
    "The p-value is a probability value, so it ranges between 0 and 1. The p-value shows us the probability of having obtained the result that we have obtained assuming that the null hypothesis H0 is true. It is often said that high values ​​of p do not allow H0 to be rejected, while low p values ​​do allow H0 to be rejected.\n",
    "\n",
    "In a statistical test, the null hypothesis H0 is rejected if the p-value associated with the observed result is equal to or less than an arbitrarily set $\\alpha$ significance level, conventionally 0.05 or 0.01. In other words, if the result obtained is more unusual than the expected range of results given a true null hypothesis H0 and the chosen significance level $\\alpha$, that is, if p is less than $\\alpha$, we can say that we have a statistically significant result that allows reject H0.\n",
    "\n",
    "It is important to emphasize that a hypothesis test does not allow accepting a hypothesis; he simply rejects it or does not reject it, that is to say, he dismisses it as plausible (which does not necessarily mean that it is true, simply that it is more likely to be) or implausible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a84594",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Of course, the hours one studies are not the only important factor in getting good grades in the real world. We can think of IQ, for example, as another determining factor. In fact, we can generalize a linear model to have as many variables as we want:\n",
    "\n",
    " $$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_2 X_m + \\epsilon$$\n",
    " \n",
    " In this case, what we are going to do is add a variable that subtracts from the note, the party hours.\n",
    " Imagine that for every hour that we go out partying, neurons in our brain die and we forget information, therefore, we will lose a grade (remember that we are inventing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f09ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "247b3ea8",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The multiple linear regression coefficients are computed in a similar way to the simple linear regression case: they minimize\n",
    "\n",
    "$$RSS = \\Sigma(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    " $$ \\hat{y} = \\hat{beta_0} + \\hat{beta_1 X_1} + \\hat{beta_2} X_2 + \\hat + \\hat{\\beta_2} X_m$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21fe3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5d27de8",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The coefficient is the number by which we multiply the predictor variable (X) in this case we have two, study hours and party hours.\n",
    "The positive coefficient indicates that the correlation is positive (the more hours I study, the better the grade I get) and the coefficient of party hours is negative, therefore it indicates that if that predictive variable is greater, my target variable decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca041d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b31a870d",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Categorical variables\n",
    "\n",
    "Very often we are faced with situations where the predictors are *qualitative* in nature. A good example could be the music they listen to which can take the values ​​$rock$ or $indie$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7481276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22fdb526",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "We include this information in the model through a *dummy* variable:\n",
    "$$\n",
    "x_i= \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1  \\quad \\text{if listens to rock} \\\\\n",
    "      0  \\quad \\text{if listens to indie} \\\\\n",
    "\\end{array} \n",
    "\\right. \n",
    "$$\n",
    "\n",
    "\n",
    "If this is our only variable, this results in a model:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\beta_0 + \\beta_1 +\\epsilon_i  \\quad \\text{if listens to rock} \\\\\n",
    "      \\beta_0 + \\epsilon_i  \\quad \\text{if listens to indie} \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "In this case, $\\beta_0$ represents the average score of people who listen to rock, and $\\beta_0 + \\beta_1$ the average score of people who listen to indie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-scheduling",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "888c5380",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Linear model extensions\n",
    "\n",
    "There are several assumptions used when fitting a linear model.\n",
    "Linear model assumptions \n",
    "[VIDEO](https://www.youtube.com/watch?v=hVe2F9krrWk)\n",
    "\n",
    "* Errors are normally distributed and have constant variance: luxury items\n",
    "* Errors are not correlated with each other\n",
    "\n",
    "***Additive Assumption** The effect of changes in a predictor $X_j$ on the response $Y$ is independent of the values ​​of the other predictors.\n",
    "\n",
    "***Linear Assumption**The change in the answer for a unit increase in $X_j$ is the same regardless of the value of $X_j$.\n",
    "\n",
    "### Challenging the additive assumption: synergy\n",
    "\n",
    "Sometimes our variables will have natural interactions. For example, we may think that the more our ads are heard on the radio, the more effective our TV ads will be. That is, the effect of both is *greater* (or *lesser*) than the sum of the parts.\n",
    "\n",
    "This is a commonly studied topic in [marketing](https://smallbusiness.chron.com/definition-synergy-marketing-21786.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963cc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ece35f",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Let's do it\n",
    "* Create three independent simple linear regression models\n",
    "* Interpret the results\n",
    "* Create a multivariate model with the three predictors\n",
    "* Interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-patent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bb4540c",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "The difference is that the covariance gives us the direction (positive or negative) between the variables and the correlation gives us this plus the strength of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcfd1e4",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Remember the **hierarchical principle:**\n",
    "\n",
    "\"*If we include an interaction in a model, we must also include the main effects, even if the p-values ​​associated with its coefficients are not significant*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ff73f",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### $R^2$ Adjusted\n",
    "There is a curious thing with $R^2$. Look what happens when we include *random* variables!\n",
    "\n",
    "\"It decreases when a predictor improves the model by less than expected. Typically, the adjusted R-squared is positive, not negative. It is always lower than the R-squared.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d252c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "24d24b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adv[\"rand_1\"] = np.random.normal(size=200)\n",
    "df_adv[\"rand_2\"] = np.random.normal(size=200)\n",
    "df_adv[\"rand_3\"] = np.random.normal(size=200)\n",
    "df_adv[\"rand_4\"] = np.random.normal(size=200)\n",
    "df_adv[\"rand_5\"] = np.random.normal(size=200)\n",
    "df_adv[\"rand_6\"] = np.random.normal(size=200)\n",
    "df_adv[\"rand_7\"] = np.random.normal(size=200)\n",
    "df_adv[\"rand_8\"] = np.random.normal(size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c28d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "348bd1c0",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Potential problems in linear regression\n",
    "\n",
    "The main assumptions of a linear model are:\n",
    "\n",
    "* Data is linear\n",
    "* Errors are not correlated\n",
    "* The variance of the error terms is constant\n",
    "\n",
    "What happens if these assumptions are not met?\n",
    "\n",
    "In addition, our models can suffer from other problems such as:\n",
    "* Outliers\n",
    "* Collinearity\n",
    "* Missing values\n",
    "\n",
    "See this [video](https://www.youtube.com/watch?v=hVe2F9krrWk) for an introduction to the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c703b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2759e222",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "These four data sets are different, but they happen to have the same arithmetic mean and variance of the x and y values, the same correlation, the same correlation coefficient, and the same regression line. some with 2 or 3 decimal places. They are the Anscombe Quartet, named for F.J. Anscombe, a statistical mathematician who published them in 1973. They are often used to teach that in addition to calculating the statistical properties of data, it is convenient to visualize them.\n",
    "\n",
    "In all cases, the representations tell us something more about the data: the first ones seem somewhat random but related, the second ones show a clear but remarkably different pattern; in the third and fourth there are other patterns clouded by some outliers. These values ​​can be errors, real data that is just out of the ordinary, or even artificially produced data to make it all fit together.\n",
    "\n",
    "Moral: don't blindly trust the data and neither the statistics you get from them; also try to set up a visualization to understand them.\n",
    "![anscombe](../images/anscombe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3344eca",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62995837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "competitive-enough",
   "metadata": {},
   "source": [
    "## Further Materials \n",
    "\n",
    "* One example of [linear regression with the Boston data set](https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-a",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.007px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
